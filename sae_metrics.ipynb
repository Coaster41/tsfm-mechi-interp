{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ce17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import (HookedTransformer, utils)\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import functools\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda:7' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import PatchTSTForPrediction\n",
    "from transformers.models.patchtst.modeling_patchtst import (\n",
    "    PatchTSTForPredictionOutput\n",
    ")\n",
    "from data_loader import *\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    upload_saes_to_huggingface,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    TimeSeriesModelSAERunnerConfig,\n",
    "    TimeSeriesModelSAETrainingRunner,\n",
    "    SAETrainingRunner,\n",
    "    StandardTrainingSAEConfig,\n",
    "    LoggingConfig,\n",
    "    HookedSAETransformer,\n",
    "    ActivationsStore,\n",
    "    run_evals,\n",
    ")\n",
    "import json\n",
    "\n",
    "from sae_lens.evals import EvalConfig\n",
    "from sae_lens.util import extract_stop_at_layer_from_tlens_hook_name\n",
    "from sae_lens.training.activation_scaler import ActivationScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e48418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot fold in layer norm, normalization_type is not LN.\n",
      "WARNING:root:You are not using LayerNorm or RMSNorm, so the layer norm weights can't be folded! Skipping\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model patchtst_relu into HookedTransformer\n",
      "Moving model to device:  cuda:7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot fold in layer norm, normalization_type is not LN.\n",
      "WARNING:root:You are not using LayerNorm or RMSNorm, so the layer norm weights can't be folded! Skipping\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model patchtst_relu into HookedTransformer\n",
      "Moving model to device:  cuda:7\n",
      "üöÄ CREATING CACHED TSMIXUP DATASETS\n",
      "==================================================\n",
      "üìÇ Found existing processed data at /extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/data/tsmixup_cache/tsmixup_processed_300000_512_96.pkl\n",
      "‚ö° Loading preprocessed data from cache...\n",
      "‚úÖ Loaded 174,209 preprocessed samples\n",
      "üìÖ Cache created: 2025-08-03 15:05:17\n",
      "\n",
      "üìä DATASET SUMMARY:\n",
      "  Total processed samples: 174,209\n",
      "  Context length: 512\n",
      "  Prediction length: 96\n",
      "üîÄ Shuffling data...\n",
      "üìà Data split:\n",
      "  Training samples: 156,788\n",
      "  Validation samples: 17,421\n",
      "  Train ratio: 90.0%\n",
      "üèóÔ∏è  Creating PyTorch datasets...\n",
      "üèóÔ∏è  Dataset created with 156,788 samples\n",
      "üìä Augmentation: ON\n",
      "üìà Dataset Statistics (from 1000 samples):\n",
      "  Sequence lengths: min=608, max=2043, mean=1320\n",
      "  Value ranges: min=-49.2103, max=70.9532\n",
      "  Value stats: mean=0.9038, std=2.2952\n",
      "üèóÔ∏è  Dataset created with 17,421 samples\n",
      "üìä Augmentation: OFF\n",
      "üìà Dataset Statistics (from 1000 samples):\n",
      "  Sequence lengths: min=608, max=2048, mean=1284\n",
      "  Value ranges: min=-28.6077, max=79.0030\n",
      "  Value stats: mean=0.9878, std=1.6189\n",
      "\n",
      "üéâ DATASETS CREATED SUCCESSFULLY!\n",
      "‚úÖ Train dataset: 156,788 samples\n",
      "‚úÖ Validation dataset: 17,421 samples\n",
      "\n",
      "üß™ Testing dataset loading...\n",
      "‚úÖ Sample shapes:\n",
      "  Train - Past: torch.Size([512]), Future: torch.Size([96])\n",
      "  Val   - Past: torch.Size([512]), Future: torch.Size([96])\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"patchtst_relu\", center_unembed=False).to(device)\n",
    "sae_model = HookedSAETransformer.from_pretrained(\"patchtst_relu\", center_unembed=False).to(device)\n",
    "\n",
    "# Loading tsmixup dataset\n",
    "train_dataset, val_dataset = create_cached_tsmixup_datasets(\n",
    "        max_samples=300000,\n",
    "        context_length=512,\n",
    "        prediction_length=96, # 1 or 96\n",
    "        num_workers=16,\n",
    "        cache_dir=\"/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/data/tsmixup_cache/\",\n",
    "        processed_cache_path=\"/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/data/tsmixup_cache/tsmixup_processed_300000_512_96.pkl\",\n",
    "        batch_size=4000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f24cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sae_lens:You just passed in a dataset which will override the one specified in your configuration: autogluon/chronos_datasets. As a consequence this run will not be reproducible via configuration alone.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model patchtst_relu into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/saes/sae.py:249: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcoaster41\u001b[0m (\u001b[33mcoaster41-uci\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/tsfm-mechi-interp/wandb/run-20250814_224148-83y87tty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests/runs/83y87tty' target=\"_blank\">patchtst_relu_test</a></strong> to <a href='https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests' target=\"_blank\">https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests/runs/83y87tty' target=\"_blank\">https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests/runs/83y87tty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2840e64634824abfa309a24d68eb3f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training SAE:   0%|          | 0/4096000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:397: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:717: UserWarning: All samples in the training dataset have been exhausted, beginning new epoch.\n",
      "  warnings.warn(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:397: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/uni2ts/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:717: UserWarning: All samples in the training dataset have been exhausted, beginning new epoch.\n",
      "  warnings.warn(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:397: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/uni2ts/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:717: UserWarning: All samples in the training dataset have been exhausted, beginning new epoch.\n",
      "  warnings.warn(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:397: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/uni2ts/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:717: UserWarning: All samples in the training dataset have been exhausted, beginning new epoch.\n",
      "  warnings.warn(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:397: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/uni2ts/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:717: UserWarning: All samples in the training dataset have been exhausted, beginning new epoch.\n",
      "  warnings.warn(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/mechanistic_interpretability/SAELens/sae_lens/training/activations_store.py:397: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  yield torch.tensor(\n",
      "/extra/datalab_scratch0/ctadler/time_series_models/uni2ts/venv/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÅ</td></tr><tr><td>details/l1_coefficient</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>details/n_training_samples</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>losses/l1_loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>losses/mse_loss</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>losses/overall_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>metrics/explained_variance</td><td>‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>metrics/explained_variance_legacy</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>‚ñÅ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>metrics/l0</td><td>‚ñá‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>‚ñÅ</td></tr><tr><td>sparsity/below_1e-5</td><td>‚ñÅ</td></tr><tr><td>sparsity/below_1e-6</td><td>‚ñÅ</td></tr><tr><td>sparsity/dead_features</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0</td></tr><tr><td>details/l1_coefficient</td><td>0.5</td></tr><tr><td>details/n_training_samples</td><td>4096000</td></tr><tr><td>losses/l1_loss</td><td>0.64185</td></tr><tr><td>losses/mse_loss</td><td>0.50712</td></tr><tr><td>losses/overall_loss</td><td>1.14896</td></tr><tr><td>metrics/explained_variance</td><td>0.7203</td></tr><tr><td>metrics/explained_variance_legacy</td><td>0.67434</td></tr><tr><td>metrics/explained_variance_legacy_std</td><td>0.12833</td></tr><tr><td>metrics/l0</td><td>240.58301</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-0.93194</td></tr><tr><td>sparsity/below_1e-5</td><td>1</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>0.92041</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">patchtst_relu_test</strong> at: <a href='https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests/runs/83y87tty' target=\"_blank\">https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests/runs/83y87tty</a><br> View project at: <a href='https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests' target=\"_blank\">https://wandb.ai/coaster41-uci/patchtst_sae_metric_tests</a><br>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250814_224148-83y87tty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_training_steps = 1000  # probably we should do more\n",
    "batch_size = 4096\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "d_in = 256\n",
    "expansion_factor = 16\n",
    "num_patches = 32\n",
    "\n",
    "cfg = TimeSeriesModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"patchtst_relu\",  # my model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    dataset_path=\"autogluon/chronos_datasets\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=True,\n",
    "    dataset_dtype=\"torch.float32\",\n",
    "    streaming=False,  # we could pre-download the token dataset if it was small.\n",
    "    # SAE Parameters\n",
    "    sae=StandardTrainingSAEConfig(\n",
    "        d_in=d_in,  # the width of the mlp output.\n",
    "        d_sae=d_in * expansion_factor,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "        apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
    "        normalize_activations=\"none\",\n",
    "        l1_coefficient=0.5,  # will control how sparse the feature activations are\n",
    "        l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    ),\n",
    "    # Training Parameters\n",
    "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
    "    num_patches=num_patches,\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64*16,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size_prompts=16,\n",
    "    # Resampling protocol\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    logger=LoggingConfig(\n",
    "        log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "        wandb_project=\"patchtst_sae_metric_tests\",\n",
    "        wandb_log_frequency=10,\n",
    "        eval_every_n_wandb_logs=20,\n",
    "        run_name=f\"patchtst_relu_test\"\n",
    "    ),\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder = TimeSeriesModelSAETrainingRunner(cfg, override_dataset=val_dataset).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02746f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
